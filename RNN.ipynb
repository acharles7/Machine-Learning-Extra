{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'O', 'U', '$', '-', 'S', ',', 'P', '!', 'F', 'w', 'B', ';', 'W', 'a', 'X', 'R', 'b', 'Z', 'p', 't', 'V', '.', 'n', 'h', 'k', 'Y', 'o', '3', 'A', 's', 'v', 'e', '[', 'j', 'H', 'd', 'q', '&', 'D', 'K', 'C', ' ', 'l', 'c', 'g', 'N', 'y', 'z', 'G', 'r', 'E', 'm', '\\n', 'x', \"'\", 'Q', ':', 'J', ']', 'M', 'f', 'L', 'I', 'u', '?', 'i']\n",
      "4573338\n"
     ]
    }
   ],
   "source": [
    "data = open(\"shakespeare.txt\", \"r\").read()\n",
    "chars = list(set(data))\n",
    "print(chars)\n",
    "print(len(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i, ch in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 67)\n",
      "(100, 100)\n",
      "(67, 100)\n"
     ]
    }
   ],
   "source": [
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))\n",
    "print(Wxh.shape)\n",
    "print(Whh.shape)\n",
    "print(Why.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = 0\n",
    "# print(data[p:p+seq_length])\n",
    "# print(data[p+1:p+seq_length+1])\n",
    "# inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "# targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "# hprev = np.zeros((hidden_size, 1))\n",
    "\n",
    "# print(inputs)\n",
    "# print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS: 100\n"
     ]
    }
   ],
   "source": [
    "def lossFunction(inputs, targets, hprev):\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    loss = 0\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Whh, hs[t-1]) + np.dot(Wxh, xs[t]) + bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + by\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) \n",
    "        loss += -np.log(ps[t][targets[t], 0])\n",
    "#     print(\"---XS---\",xs)\n",
    "        #print(\"Probs\",ps[t])\n",
    "    print(\"HS:\",len(hs[0]))\n",
    "#     print(\"HS:\",hs)\n",
    "\n",
    "    \n",
    "    mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    for t in reversed(range(len(inputs))):\n",
    "#         print(\"back iter:\",t)\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    \n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]        \n",
    "        \n",
    "loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFunction(inputs, targets, hprev)  \n",
    "\n",
    "# print(dWxh.shape)\n",
    "# print(dWhh.shape)\n",
    "# print(dWhy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(h, seed_ix, n):\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 105.075825\n",
      "iter 10, loss: 105.605261\n",
      "iter 20, loss: 105.573109\n",
      "iter 30, loss: 105.394430\n",
      "iter 40, loss: 105.188741\n",
      "iter 50, loss: 104.993643\n",
      "iter 60, loss: 104.724870\n",
      "iter 70, loss: 104.441059\n",
      "iter 80, loss: 104.167545\n",
      "iter 90, loss: 103.963470\n",
      "iter 100, loss: 103.762445\n",
      "iter 110, loss: 103.656251\n",
      "iter 120, loss: 103.408178\n",
      "iter 130, loss: 103.143661\n",
      "iter 140, loss: 102.933745\n",
      "iter 150, loss: 102.723364\n",
      "iter 160, loss: 102.479413\n",
      "iter 170, loss: 102.267698\n",
      "iter 180, loss: 102.026273\n",
      "iter 190, loss: 101.835425\n",
      "iter 200, loss: 101.568291\n",
      "iter 210, loss: 101.353747\n",
      "iter 220, loss: 101.151835\n",
      "iter 230, loss: 100.941359\n",
      "iter 240, loss: 100.661483\n",
      "iter 250, loss: 100.419059\n",
      "iter 260, loss: 100.149840\n",
      "iter 270, loss: 99.890724\n",
      "iter 280, loss: 99.645581\n",
      "iter 290, loss: 99.349035\n",
      "iter 300, loss: 99.148149\n",
      "iter 310, loss: 98.858901\n",
      "iter 320, loss: 98.596433\n",
      "iter 330, loss: 98.333540\n",
      "iter 340, loss: 98.053045\n",
      "iter 350, loss: 97.799420\n",
      "iter 360, loss: 97.577901\n",
      "iter 370, loss: 97.349143\n",
      "iter 380, loss: 97.060070\n",
      "iter 390, loss: 96.810341\n",
      "iter 400, loss: 96.643037\n",
      "iter 410, loss: 96.450394\n",
      "iter 420, loss: 96.244510\n",
      "iter 430, loss: 95.999717\n",
      "iter 440, loss: 95.795907\n",
      "iter 450, loss: 95.599496\n",
      "iter 460, loss: 95.382049\n",
      "iter 470, loss: 95.238490\n",
      "iter 480, loss: 94.990573\n",
      "iter 490, loss: 94.750658\n",
      "iter 500, loss: 94.549297\n",
      "iter 510, loss: 94.290388\n",
      "iter 520, loss: 94.066907\n",
      "iter 530, loss: 93.826153\n",
      "iter 540, loss: 93.556768\n",
      "iter 550, loss: 93.347191\n",
      "iter 560, loss: 93.124513\n",
      "iter 570, loss: 92.913537\n",
      "iter 580, loss: 92.677855\n",
      "iter 590, loss: 92.503073\n",
      "iter 600, loss: 92.279492\n",
      "iter 610, loss: 92.048753\n",
      "iter 620, loss: 91.853113\n",
      "iter 630, loss: 91.613493\n",
      "iter 640, loss: 91.493791\n",
      "iter 650, loss: 91.293143\n",
      "iter 660, loss: 91.149734\n",
      "iter 670, loss: 91.008702\n",
      "iter 680, loss: 90.846330\n",
      "iter 690, loss: 90.662543\n",
      "iter 700, loss: 90.491158\n",
      "iter 710, loss: 90.343544\n",
      "iter 720, loss: 90.174317\n",
      "iter 730, loss: 89.954688\n",
      "iter 740, loss: 89.787940\n",
      "iter 750, loss: 89.588495\n",
      "iter 760, loss: 89.436546\n",
      "iter 770, loss: 89.231645\n",
      "iter 780, loss: 89.025004\n",
      "iter 790, loss: 88.812947\n",
      "iter 800, loss: 88.576797\n",
      "iter 810, loss: 88.353095\n",
      "iter 820, loss: 88.156654\n",
      "iter 830, loss: 87.946696\n",
      "iter 840, loss: 87.727572\n",
      "iter 850, loss: 87.489555\n",
      "iter 860, loss: 87.291397\n",
      "iter 870, loss: 87.055074\n",
      "iter 880, loss: 86.819937\n",
      "iter 890, loss: 86.621073\n",
      "iter 900, loss: 86.420927\n",
      "iter 910, loss: 86.233431\n",
      "iter 920, loss: 86.039598\n",
      "iter 930, loss: 85.850781\n",
      "iter 940, loss: 85.635893\n",
      "iter 950, loss: 85.448671\n",
      "iter 960, loss: 85.287220\n",
      "iter 970, loss: 85.090773\n",
      "iter 980, loss: 84.903254\n",
      "iter 990, loss: 84.736928\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "while(n<1000):\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p+seq_length+1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "        p = 0 # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # sample from the model now and then\n",
    "#     if n % 100 == 0:\n",
    "#         sample_ix = sample(hprev, inputs[0], 200)\n",
    "#         txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "#         print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFunction(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 10 == 0: \n",
    "        print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "  \n",
    "  # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by], \n",
    "                                [dWxh, dWhh, dWhy, dbh, dby], \n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update\n",
    "\n",
    "    p += seq_length # move data pointer\n",
    "    n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation\n",
    "\n",
    "#### error = (Actual Output - Predicted Output)^2\n",
    "#### delta w = learning rate * (de/dw)\n",
    "#### W = w + delta w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Character Tokenizations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "n = 0\n",
    "X = []\n",
    "Y = []\n",
    "while (n < 100):\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "    X.append(inputs)\n",
    "    Y.append(targets)\n",
    "    p += seq_length\n",
    "    n += 1\n",
    "print(len(X))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "with open('X_100.txt', 'w') as fx:\n",
    "    for item in X:\n",
    "        string = re.sub('[\\[\\]]','',repr(item))\n",
    "        fx.write(\"%s\\n\" % string)\n",
    "with open('Y_100.txt', 'w') as fy:\n",
    "    for item in Y:\n",
    "        string = re.sub('[\\[\\]]','',repr(item))\n",
    "        fy.write(\"%s\\n\" % string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
